{
 "metadata": {
  "name": "",
  "signature": "sha256:cc7dcac74f59f038dc2e3cbaf2b5b37d14afee86d214aedc105aae0108d4492b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Binning\n",
      "Data pre-processing technique to reduce minor observation errors and reduce data size\n",
      "\n",
      "By [Martin A. Fitzpatrick](http://martinfitzpatrick.name/)\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Data binning is a widely used data pre-processing technique that can be used to reduce the effects of minor observational errors.\n",
      "It can also be used to reduce the size of a dataset for easier processing. Regions of the data (bins) are defined, within which\n",
      "data is replaced with a representative value, e.g. a mean, median or central value. It is a form of quantization.\n",
      "\n",
      "It is important to remember that binning results in a loss of data, and may introduce artefacts. This plugin attempts to \n",
      "mitigate some of these problems by providing information on data loss, and offering adaptive binning approaches.\n",
      "\n",
      "## Quick start\n",
      "\n",
      "This plugin can bin any data in 1 dimension, e.g. NMR spectra. Simply select the data source and the data will be automatically binned.\n",
      "You can adjust the bin size using the toolbar, together with a bin offset. You can view the result of the binning a tabular, or \n",
      "spectral view. Importantly, this plugin also presents a difference view that shows the data loss associated with a given binning strategy, \n",
      "together with a summary view.\n",
      "\n",
      "## Tip\n",
      "\n",
      "Resulting data output can be fed into statistical analysis, for example PCA, where the effects of binning can be observed live."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import copy\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "dsi = input\n",
      "\n",
      "###### BINNING USING CONFI\n",
      "# Generate bin values for range start_scale to end_scale\n",
      "# Calculate the number of bins at binsize across range\n",
      "dso = copy.deepcopy(dsi)\n",
      "dso.import_data(dsi)\n",
      "\n",
      "r = dsi.scales_r[1]\n",
      "\n",
      "self._bin_size, self._bin_offset = self.config.get('bin_size'), self.config.get('bin_offset')\n",
      "\n",
      "bins = np.arange(r[0] + self._bin_offset, r[1] + self._bin_offset, self._bin_size)\n",
      "number_of_bins = len(bins) - 1\n",
      "\n",
      "# Can't increase the size of data, if bins > current size return the original\n",
      "if number_of_bins >= len(dso.scales[1]):\n",
      "    return {'dso': dso}\n",
      "\n",
      "# Resize (lossy) to the new shape\n",
      "old_shape, new_shape = list(dsi.data.shape), list(dso.data.shape)\n",
      "new_shape[1] = number_of_bins\n",
      "dso.crop(new_shape)  # Lossy crop, but we'll be within the boundary below\n",
      "\n",
      "\n",
      "for n, d in enumerate(dsi.data):\n",
      "    binned_data = np.histogram(dsi.scales[1], bins=bins, weights=d)\n",
      "    binned_num = np.histogram(dsi.scales[1], bins=bins)  # Number of data points that ended up contributing to each bin\n",
      "    dso.data[n, :] = binned_data[0] / binned_num[0]  # Mean\n",
      "\n",
      "dso.scales[1] = [float(x) for x in binned_data[1][:-1]]\n",
      "#dso.labels[1] = [str(x) for x in binned_data[1][:-1]]\n",
      "\n",
      "# Remove any NaNs that have crept in (due to the histogram)\n",
      "dso.remove_invalid_data()\n",
      "\n",
      "output = dso;"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}